services:
  # LiveKit server - Real-time communication server for WebRTC
  livekit:
    image: livekit/livekit-server:latest
    command: --config /etc/livekit.yaml
    restart: always
    ports:
      - "17880:17880"     # WebSocket/HTTP API
      - "17881:17881"     # TCP for signaling
      - "50700-50720:50700-50720/udp" # UDP for media (WebRTC)
    volumes:
      - ./livekit.yaml:/etc/livekit.yaml  # Mount LiveKit configuration
    networks:
      - livekit-gpu-network
    depends_on:
      redis:
        condition: service_started

  # GPU Avatar Worker - Self-hosted GPU rendering service
  # This service runs the expression-avatar container for local GPU rendering
  #
  # Prerequisites:
  #   Model weights are provided by bitHuman upon access approval.
  #   Place the provided model files in ./models/ before starting.
  #   Expected layout (~11.5 GB total):
  #     ./models/bithuman-expression/Model_Lite/   ← DiT model weights (5.7 GB)
  #     ./models/bithuman-expression/VAE_LTX/      ← video VAE (1.6 GB)
  #     ./models/wav2vec2-base-960h/               ← audio encoder (1.1 GB)
  #     ./models/dit_lite_fp16.trt                 ← TRT engine (2.9 GB)
  #     ./models/turbo-vaed/                       ← fast VAE decoder (162 MB)
  expression-avatar:
    image: docker.io/sgubithuman/expression-avatar:latest
    restart: always
    # GPU support using Docker Compose v2 syntax
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - BITHUMAN_API_SECRET=${BITHUMAN_API_SECRET}
      - CUDA_VISIBLE_DEVICES=0  # Use first GPU
    volumes:
      # Mount directory containing model weights provided by bitHuman
      - ./models:/data/models:ro
    networks:
      - livekit-gpu-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8089/health')"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 120s  # ~48s GPU compilation + buffer

  # AI Agent (GPU) - Backend service with custom GPU endpoint support
  agent-gpu:
    build:
      context: .  # Build from current directory
      dockerfile: agent.gpu.dockerfile
    entrypoint: ["python", "agent.py", "start"]
    restart: always
    environment:
      # Avatar mode -- unified agent.py supports both cpu and gpu
      - AVATAR_MODE=gpu
      # LiveKit connection settings
      - LIVEKIT_URL=ws://livekit:17880  # Internal Docker network URL
      - LIVEKIT_API_KEY=devkey  # Default dev key (change in production)
      - LIVEKIT_API_SECRET=UPivas8PQvWiYhubkqxqfkY8kfB9TgGj
      # Custom GPU endpoint settings
      - CUSTOM_GPU_URL=http://expression-avatar:8089/launch  # Internal Docker network URL
      - CUSTOM_GPU_TOKEN=${CUSTOM_GPU_TOKEN:-}  # Optional token for GPU worker auth
      # Avatar settings -- set AVATAR_ID or BITHUMAN_AVATAR_IMAGE in .env.gpu (one is required)
      - AVATAR_ID=${AVATAR_ID:-}  # Pre-configured avatar ID on the worker
      - BITHUMAN_AVATAR_IMAGE=${BITHUMAN_AVATAR_IMAGE:-}  # Local path or URL (e.g. /app/avatars/avatar.jpg)
      - OPENAI_VOICE=${OPENAI_VOICE:-alloy}  # Voice for OpenAI realtime
      - AVATAR_PERSONALITY=${AVATAR_PERSONALITY:-}  # Optional: Custom personality prompt
    env_file:
      - ./.env.gpu  # API keys (BITHUMAN_API_SECRET, OPENAI_API_KEY) + avatar config
    volumes:
      # Mount avatars directory for custom avatar images
      # Place your avatar images in ./avatars on host, then set BITHUMAN_AVATAR_IMAGE=/app/avatars/your-image.jpg
      - ./avatars:/app/avatars
      - ./agent.py:/app/agent.py  # Mount for live editing
    networks:
      - livekit-gpu-network
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Access host services
    depends_on:
      livekit:
        condition: service_started
      # Note: expression-avatar is not required for agent to connect to LiveKit
      # Agent will connect immediately, and GPU worker will be called when needed
      # expression-avatar:
      #   condition: service_healthy

  # Frontend - Web interface (LiveKit agents playground)
  frontend:
    build:
      context: .
      dockerfile: webui.dockerfile
      args:
        # Build-time: NEXT_PUBLIC_LIVEKIT_URL is embedded in client code
        # For remote deployment, set this to the public IP/domain, e.g.:
        # LIVEKIT_URL: ws://34.22.109.207:17880
        # For local deployment, use:
        LIVEKIT_URL: ws://localhost:17880  # Public URL for browser access
    restart: always
    ports:
      - "4202:4202"
    environment:
      # Runtime environment variables (for server-side API routes)
      - LIVEKIT_API_KEY=devkey  # API key for LiveKit connection
      - LIVEKIT_API_SECRET=UPivas8PQvWiYhubkqxqfkY8kfB9TgGj
      - PORT=4202  # Override Next.js default port
      # Note: NEXT_PUBLIC_* variables are embedded at build time
      # If you need to change the URL after build, you must rebuild the image
      # with the correct LIVEKIT_URL build arg
    networks:
      - livekit-gpu-network
    depends_on:
      livekit:
        condition: service_started

  # Redis - Message broker for LiveKit clustering and state management
  redis:
    image: redis:alpine
    restart: always
    networks:
      - livekit-gpu-network
    # No external ports - only used internally by LiveKit

# Docker network for service communication
networks:
  livekit-gpu-network:
    driver: bridge  # Default Docker bridge network

# Docker volumes for persistent data
volumes: {}
