---
title: "Self-Hosted GPU Container"
description: "Deploy GPU avatar workers on your infrastructure with full control over scaling, costs, and data privacy."
icon: "gpu-card"
---

<Info>
**Preview Feature** — 2 credits per minute while using the GPU container.
</Info>

## Overview

The self-hosted GPU avatar container (`docker.io/sgubithuman/expression-avatar:latest`) enables production-grade avatar generation on your own GPU infrastructure.

- **Full Control** — Complete control over deployment, scaling, and configuration
- **Cost Optimization** — Pay only for the GPU resources you use
- **Data Privacy** — Avatar images and audio never leave your infrastructure
- **Customization** — Extend the worker with custom logic and integrations

### How It Works

The container is a GPU worker that joins a [LiveKit](https://livekit.io) room and streams avatar video frames in real time. Your application calls the `/launch` endpoint with LiveKit room credentials and an avatar image; the container connects to the room, listens for audio, and generates lip-synced video at 25 FPS — entirely on your GPU.

```
Your Agent (LiveKit)
      │
      │  POST /launch
      │  { livekit_url, livekit_token, room_name, avatar_image }
      ▼
expression-avatar container
      │
      ├─ Joins LiveKit room as video publisher
      ├─ Receives audio from agent via data stream
      └─ Generates 25 FPS lip-synced video → streams to room
             ↑
         100% local GPU — no cloud calls during inference
```

---

## Prerequisites

- NVIDIA GPU with **≥8 GB VRAM** (tested on H100, A100, RTX 4090)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) installed
- Docker 20.10+ or Docker Compose v2
- bitHuman API secret (`sk_bh_...`) from the [bitHuman Console](https://www.bithuman.ai)
- Model weights package from bitHuman (see below)
- A running [LiveKit server](https://docs.livekit.io/home/self-hosting/local/) (or LiveKit Cloud)

---

## Getting Model Weights

Model weights are provided by bitHuman as part of the Self-Hosted GPU access program.

<Info>
[Contact us](https://www.bithuman.ai) to request access and receive the model weights package.
</Info>

Once received, extract the weights so your directory looks like:

```
models/
├── bithuman-expression/
│   ├── Model_Lite/         ← DiT model weights (5.7 GB)
│   └── VAE_LTX/            ← video VAE (1.6 GB)
├── wav2vec2-base-960h/     ← audio encoder (1.1 GB)
├── dit_lite_fp16.trt       ← TRT engine (2.9 GB)
└── turbo-vaed/             ← fast VAE decoder (162 MB)

Total: ~11.5 GB
```

---

## Quick Start

```bash
# 1. Pull the image
docker pull docker.io/sgubithuman/expression-avatar:latest

# 2. Run with your model weights and API secret
docker run --gpus all -p 8089:8089 \
    -v /path/to/models:/data/models:ro \
    -e BITHUMAN_API_SECRET=sk_bh_... \
    docker.io/sgubithuman/expression-avatar:latest
```

```bash
# 3. Wait ~48s for startup (GPU compilation), then check health
curl http://localhost:8089/health
# {"status": "healthy", "service": "expression-avatar", "active_sessions": 0, "max_sessions": 8}
```

Once healthy, the container is ready to accept avatar sessions via `/launch`.

---

## Docker Compose Setup

Use the [full example](https://github.com/bithuman-product/examples/tree/main/public-docker-example) for a complete setup with LiveKit, an AI agent, and a web frontend:

```bash
git clone https://github.com/bithuman-product/examples.git
cd examples/public-docker-example

# Create .env.gpu with your credentials
echo "BITHUMAN_API_SECRET=sk_bh_..." > .env.gpu
echo "OPENAI_API_KEY=sk-..." >> .env.gpu

# Set your avatar image (local path or public URL):
echo "BITHUMAN_AVATAR_IMAGE=/app/avatars/avatar.jpg" >> .env.gpu
# or: echo "BITHUMAN_AVATAR_IMAGE=https://example.com/avatar.jpg" >> .env.gpu

# Place model weights in ./models/
# Expected layout:
#   models/bithuman-expression/Model_Lite/
#   models/bithuman-expression/VAE_LTX/
#   models/wav2vec2-base-960h/
#   models/dit_lite_fp16.trt
#   models/turbo-vaed/

# Copy your avatar image into ./avatars/
mkdir -p avatars
cp /path/to/your/avatar.jpg avatars/

docker compose -f docker-compose.gpu.yml up
```

Open `http://localhost:4202` to start a conversation with your GPU avatar.

---

## Integration Guide

The container exposes a simple HTTP API. Your LiveKit agent calls `/launch` to start an avatar session. There are two ways to integrate:

### Option 1: LiveKit Python Plugin (Recommended)

Install the bitHuman LiveKit plugin:

```bash
pip install livekit-plugins-bithuman
```

In your LiveKit agent, point `AvatarSession` at your container's `/launch` endpoint:

```python
from livekit.agents import Agent, AgentSession, JobContext, WorkerOptions, WorkerType, cli
from livekit.plugins import bithuman, openai, silero

async def entrypoint(ctx: JobContext):
    await ctx.connect()
    await ctx.wait_for_participant()

    avatar = bithuman.AvatarSession(
        api_url="http://localhost:8089/launch",   # your container
        api_secret="sk_bh_...",                   # for billing
        avatar_image="/path/to/avatar.jpg",        # local file or HTTPS URL
    )

    session = AgentSession(
        llm=openai.realtime.RealtimeModel(voice="coral"),
        vad=silero.VAD.load(),
    )

    await avatar.start(session, room=ctx.room)
    await session.start(
        agent=Agent(instructions="You are a helpful assistant."),
        room=ctx.room,
    )

if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint, worker_type=WorkerType.ROOM))
```

The plugin handles room token generation and calls `/launch` automatically when a participant joins.

### Option 2: Direct HTTP API

You can call `/launch` directly from any HTTP client. The container joins the LiveKit room as a video publisher.

```bash
# Generate a LiveKit room token first (using livekit-server-sdk or CLI)
TOKEN=$(livekit-token create --room my-room --identity avatar-worker \
    --api-key devkey --api-secret your-livekit-secret)

# Launch with an image URL
curl -X POST http://localhost:8089/launch \
  -F "livekit_url=ws://your-livekit-server:7880" \
  -F "livekit_token=$TOKEN" \
  -F "room_name=my-room" \
  -F "avatar_image_url=https://example.com/avatar.jpg"

# Or upload an image file directly
curl -X POST http://localhost:8089/launch \
  -F "livekit_url=ws://your-livekit-server:7880" \
  -F "livekit_token=$TOKEN" \
  -F "room_name=my-room" \
  -F "avatar_image=@./avatar.jpg"
```

Response (async by default):
```json
{
  "status": "pending",
  "task_id": "a1b2c3d4",
  "room_name": "my-room"
}
```

The avatar is live in the room within ~4–6 seconds.

---

## HTTP API Reference

All endpoints are served on port `8089` (default).

### `POST /launch`

Start an avatar session for a LiveKit room. The container joins the room and begins streaming lip-synced video.

**Content-Type:** `multipart/form-data`

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `livekit_url` | string | Yes | LiveKit server WebSocket URL (e.g. `ws://livekit:7880`) |
| `livekit_token` | string | Yes | LiveKit room token with publish permissions |
| `room_name` | string | Yes | LiveKit room name (must match token) |
| `avatar_image` | file | No* | Avatar image file upload (JPEG/PNG) |
| `avatar_image_url` | string | No* | Avatar image HTTPS URL (alternative to file upload) |
| `prompt` | string | No | Motion prompt (default: `"A person is talking naturally."`) |
| `api_secret` | string | No | Override billing secret (defaults to `BITHUMAN_API_SECRET`) |
| `async_mode` | bool | No | Return immediately (`true`, default) or wait for session to end |

*Provide either `avatar_image` or `avatar_image_url`. If neither is given, a default image is used.

**Response (async_mode=true):**
```json
{ "status": "pending", "task_id": "a1b2c3d4", "room_name": "my-room" }
```

**Error responses:**
- `503 Service Unavailable` — container still initializing, or at session capacity
- `400 Bad Request` — invalid image or download failed

---

### `GET /health`

Lightweight health check. Always returns 200 once the container is running (even during model loading).

```json
{
  "status": "healthy",
  "service": "expression-avatar",
  "active_sessions": 2,
  "max_sessions": 8
}
```

---

### `GET /ready`

Readiness check. Returns `200` only when the model is loaded and a session slot is available. Use this to gate traffic in load balancers or health checks.

```json
{
  "status": "ready",
  "model_ready": true,
  "active_sessions": 2,
  "available_sessions": 6,
  "max_sessions": 8
}
```

Returns `503` with `"status": "not_ready"` during model loading, or `"status": "at_capacity"` when all session slots are in use.

---

### `GET /tasks/{task_id}`

Check the status of a specific session.

```json
{
  "task_id": "a1b2c3d4",
  "room_name": "my-room",
  "status": "running",
  "created_at": "2024-01-01T12:00:00",
  "completed_at": null,
  "error": null
}
```

Status values: `pending` → `running` → `completed` / `failed` / `cancelled`

---

### `POST /tasks/{task_id}/stop`

Stop a running session and release the session slot.

```bash
curl -X POST http://localhost:8089/tasks/a1b2c3d4/stop
```

---

### `GET /benchmark`

Run an inference benchmark and return per-stage timing. Useful for verifying GPU performance.

```bash
curl "http://localhost:8089/benchmark?iterations=10"
```

```json
{
  "iterations": 10,
  "frames_per_generate": 24,
  "avg_ms": 79.3,
  "fps": 302.6,
  "stages": {
    "dit_ms": 41.2,
    "vae_decode_ms": 13.1,
    "vae_encode_ms": 8.5,
    "color_correct_ms": 6.1,
    "postprocess_ms": 2.8,
    "audio_ms": 7.1
  },
  "vram_gb": 6.2,
  "gpu": "NVIDIA H100 80GB HBM3"
}
```

---

### `GET /test-frame`

Generate a few chunks and return the last frame as a JPEG. Useful for verifying the model is producing valid output.

```bash
curl http://localhost:8089/test-frame --output frame.jpg
open frame.jpg
```

---

## Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `BITHUMAN_API_SECRET` | Yes | — | API secret for billing (`sk_bh_...`) |
| `MAX_SESSIONS` | No | `8` | Max concurrent avatar sessions |
| `CUDA_VISIBLE_DEVICES` | No | all GPUs | Restrict to specific GPU (e.g. `0`) |
| `FAST_DECODER_CONFIG` | No | — | Path to fast decoder config JSON (optional speedup) |
| `FAST_DECODER_CHECKPOINT` | No | — | Path to fast decoder weights (optional speedup) |

<Warning>
Without `BITHUMAN_API_SECRET`, avatar sessions will run but usage will not be tracked or billed. This is not permitted for production use.
</Warning>

---

## Performance Characteristics

| GPU | FPS | VRAM | Sessions |
|-----|-----|------|----------|
| H100 80GB | ~305 FPS | ~6 GB | up to 8 concurrent |
| A100 40GB | ~200 FPS | ~6 GB | up to 6 concurrent |
| RTX 4090 | ~120 FPS | ~6 GB | up to 4 concurrent |

| Configuration | Time to First Frame | Description |
|---------------|---------------------|-------------|
| Long-running container | ~4–6 seconds | Model loaded at startup; new sessions encode image (~2s) then stream |
| Cold start | ~48 seconds | Full GPU model compilation on first start (cached on subsequent starts) |

### Long-Running Containers (Recommended)

Keep the container running between sessions. The model loads once at startup (~48s including GPU compilation), and subsequent sessions start in ~4–6 seconds.

```bash
docker run --gpus all -p 8089:8089 --restart always \
    -v /path/to/models:/data/models:ro \
    -e BITHUMAN_API_SECRET=sk_bh_... \
    docker.io/sgubithuman/expression-avatar:latest
```

---

## Troubleshooting

| Problem | Solution |
|---------|----------|
| Container won't start | Check GPU: `nvidia-smi`; verify model volume mount; check logs: `docker logs <id>` |
| `/health` returns connection refused | Container still initializing — wait for `PREWARM: Pipeline loaded` in logs |
| `/launch` returns `503 not_ready` | Model still loading — poll `/ready` until `model_ready: true` |
| `/launch` returns `503 at_capacity` | All session slots in use; increase `MAX_SESSIONS` or scale horizontally |
| Startup takes >2 minutes | GPU compilation runs once per container — subsequent starts reuse compiled cache |
| Out of memory | Use a GPU with ≥8 GB VRAM; reduce `MAX_SESSIONS` if needed |
| Billing not working | Verify `BITHUMAN_API_SECRET` is set; check logs for `[HEARTBEAT]` messages |
| Models not found | Ensure volume mount is `-v /path/to/models:/data/models:ro` with all 3 required items |
| Avatar image not showing | Check `/test-frame` — if it returns a valid JPEG, image encoding is working |

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Docker Example" icon="docker" href="https://github.com/bithuman-product/examples/tree/main/public-docker-example">
    Full GPU setup with LiveKit server, AI agent, and web frontend
  </Card>
  <Card title="LiveKit Agents" icon="book" href="https://docs.livekit.io/agents">
    LiveKit Agents Python SDK documentation
  </Card>
</CardGroup>
