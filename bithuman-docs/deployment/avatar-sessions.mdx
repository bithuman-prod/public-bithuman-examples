---
title: "Avatar Sessions"
description: "Complete guide to creating avatar sessions — cloud, self-hosted, and GPU."
icon: "book-open"
---

An **AvatarSession** is how you bring a bitHuman avatar into a LiveKit room. This guide covers every way to do it, with complete working examples.

<Info>
**New to bitHuman?** Start with [How It Works](/getting-started/how-it-works) to understand the core concepts first.
</Info>

---

## Choose Your Approach

| Approach | Best For | Model Files | GPU Required | Internet Required |
|----------|----------|-------------|--------------|-------------------|
| [Cloud Plugin](#cloud-plugin) | Getting started, web apps | No | No | Yes |
| [Self-Hosted CPU](#self-hosted-cpu) | Privacy, edge devices | Yes (.imx) | No | Only for auth |
| [Self-Hosted GPU](#self-hosted-gpu) | Dynamic faces, custom images | No (uses images) | Yes | Only for auth |

---

## Prerequisites

All approaches need these basics:

<Snippet file="prerequisites.mdx" />

You also need a LiveKit server. If you don't have one:

```bash
# Option 1: LiveKit Cloud (easiest)
# Sign up at https://cloud.livekit.io — free tier available

# Option 2: Self-hosted LiveKit
docker run --rm -p 7880:7880 -p 7881:7881 -p 7882:7882/udp \
    livekit/livekit-server --dev
```

---

## Cloud Plugin

The cloud plugin runs the avatar on bitHuman's servers. You just provide an Agent ID and API secret — no model files, no GPU.

### Complete Working Example

```python
import asyncio
import os
from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    WorkerOptions,
    cli,
    llm,
)
from livekit.plugins import openai, silero, bithuman

# 1. Define your AI agent
class MyAgent(Agent):
    def __init__(self):
        super().__init__(
            instructions="""You are a helpful and friendly assistant.
            Keep responses concise — 1-2 sentences.""",
        )

# 2. Set up the session when a user connects
async def entrypoint(ctx: JobContext):
    await ctx.connect()

    # Wait for a user to join the room
    await ctx.wait_for_participant()

    # Create the avatar session (cloud-hosted)
    avatar = bithuman.AvatarSession(
        avatar_id=os.getenv("BITHUMAN_AGENT_ID"),    # e.g. "A78WKV4515"
        api_secret=os.getenv("BITHUMAN_API_SECRET"),  # e.g. "sk_bh_..."
    )

    # Create the agent session with AI components
    session = AgentSession(
        stt=openai.STT(),                 # Speech-to-text
        llm=openai.LLM(),                 # AI language model
        tts=openai.TTS(),                 # Text-to-speech
        vad=silero.VAD.load(),            # Voice activity detection
    )

    # Start everything — avatar joins the room automatically
    await session.start(
        agent=MyAgent(),
        room=ctx.room,
        room_output=avatar,               # Avatar renders the agent's speech
    )

# 3. Launch
if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

### Environment Variables

```bash
# Required
export BITHUMAN_API_SECRET="sk_bh_..."          # From www.bithuman.ai/#developer
export BITHUMAN_AGENT_ID="A78WKV4515"           # Your agent's ID
export OPENAI_API_KEY="sk-..."                   # For STT, LLM, TTS

# LiveKit connection
export LIVEKIT_URL="wss://your-project.livekit.cloud"
export LIVEKIT_API_KEY="APIxxxxxxxx"
export LIVEKIT_API_SECRET="xxxxxxxx"
```

### Run It

```bash
python agent.py dev
```

Then open [agents-playground.livekit.io](https://agents-playground.livekit.io) to connect and talk to your avatar.

### How It Works Behind the Scenes

When `session.start()` runs with `room_output=avatar`:

1. The plugin sends a request to bitHuman's cloud API
2. A cloud avatar worker receives the request
3. The worker downloads the avatar model (cached after first time)
4. The worker joins your LiveKit room as a participant named `bithuman-avatar-agent`
5. As your agent produces TTS audio, the worker generates animated video frames
6. Video is published to the room — users see the avatar speaking

<Tip>
**Essence vs Expression model:** By default, the cloud plugin uses the **Essence** (CPU) model, which works with pre-built `.imx` avatars. Add `model="expression"` to use the **Expression** (GPU) model, which supports custom face images.
</Tip>

### Using Expression Model (GPU) with Custom Image

```python
from PIL import Image

avatar = bithuman.AvatarSession(
    avatar_image=Image.open("face.jpg"),    # Any face image
    api_secret=os.getenv("BITHUMAN_API_SECRET"),
    model="expression",
)
```

---

## Self-Hosted CPU

Run the avatar entirely on your own machine using a downloaded `.imx` model file. Great for privacy and offline use.

### Complete Working Example

```python
import asyncio
import os
from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    WorkerOptions,
    cli,
    llm,
)
from livekit.plugins import openai, silero, bithuman

class MyAgent(Agent):
    def __init__(self):
        super().__init__(
            instructions="You are a helpful assistant. Keep responses brief.",
        )

async def entrypoint(ctx: JobContext):
    await ctx.connect()
    await ctx.wait_for_participant()

    # Create the avatar session (self-hosted, CPU)
    avatar = bithuman.AvatarSession(
        model_path=os.getenv("BITHUMAN_MODEL_PATH"),  # e.g. "/models/avatar.imx"
        api_secret=os.getenv("BITHUMAN_API_SECRET"),
    )

    session = AgentSession(
        stt=openai.STT(),
        llm=openai.LLM(),
        tts=openai.TTS(),
        vad=silero.VAD.load(),
    )

    await session.start(
        agent=MyAgent(),
        room=ctx.room,
        room_output=avatar,
    )

if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

### Environment Variables

```bash
# Required
export BITHUMAN_API_SECRET="sk_bh_..."
export BITHUMAN_MODEL_PATH="/path/to/avatar.imx"
export OPENAI_API_KEY="sk-..."

# LiveKit connection
export LIVEKIT_URL="wss://your-project.livekit.cloud"
export LIVEKIT_API_KEY="APIxxxxxxxx"
export LIVEKIT_API_SECRET="xxxxxxxx"
```

### How It Differs from Cloud

| Aspect | Cloud | Self-Hosted CPU |
|--------|-------|-----------------|
| Model location | bitHuman's servers | Your machine |
| Avatar parameter | `avatar_id="A78WKV4515"` | `model_path="/path/to/avatar.imx"` |
| Internet needed | Yes (always) | Only for authentication |
| First frame latency | 2-4 seconds | ~20 seconds (model load) |
| Privacy | Audio sent to cloud | Audio stays local |

### System Requirements

- **CPU:** 4+ cores (8 recommended)
- **RAM:** 8 GB minimum
- **Disk:** ~500 MB per `.imx` model
- **OS:** Linux (x64/ARM64), macOS (M2+), or Windows (WSL)

---

## Self-Hosted GPU

Use a GPU container that generates avatars from any face image — no pre-built models needed.

### Complete Working Example

```python
import asyncio
import os
from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    WorkerOptions,
    cli,
    llm,
)
from livekit.plugins import openai, silero, bithuman

class MyAgent(Agent):
    def __init__(self):
        super().__init__(
            instructions="You are a helpful assistant. Keep responses brief.",
        )

async def entrypoint(ctx: JobContext):
    await ctx.connect()
    await ctx.wait_for_participant()

    # Create the avatar session (self-hosted GPU container)
    avatar = bithuman.AvatarSession(
        api_url=os.getenv("GPU_WORKER_URL"),            # e.g. "http://localhost:8089"
        api_token=os.getenv("BITHUMAN_API_SECRET"),
        avatar_image="https://example.com/face.jpg",    # Any face image URL
    )

    session = AgentSession(
        stt=openai.STT(),
        llm=openai.LLM(),
        tts=openai.TTS(),
        vad=silero.VAD.load(),
    )

    await session.start(
        agent=MyAgent(),
        room=ctx.room,
        room_output=avatar,
    )

if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

### Start the GPU Container First

```bash
# Pull and run the GPU avatar container
docker run --gpus all -p 8089:8089 \
    -v /path/to/model-storage:/data/models \
    -e BITHUMAN_API_SECRET=sk_bh_... \
    docker.io/sgubithuman/expression-avatar:latest
```

### Environment Variables

```bash
# Required
export BITHUMAN_API_SECRET="sk_bh_..."
export GPU_WORKER_URL="http://localhost:8089"
export OPENAI_API_KEY="sk-..."

# LiveKit connection
export LIVEKIT_URL="wss://your-project.livekit.cloud"
export LIVEKIT_API_KEY="APIxxxxxxxx"
export LIVEKIT_API_SECRET="xxxxxxxx"
```

<Note>
For detailed GPU container setup, see [Self-Hosted GPU Container](/deployment/self-hosted-gpu).
</Note>

---

## Adding Gestures (Dynamics)

Make your avatar perform gestures like waving, nodding, or laughing in response to conversation keywords.

<Warning>
Dynamics require a cloud-generated agent with gestures enabled. Create one at [www.bithuman.ai](https://www.bithuman.ai).
</Warning>

### Step 1: Check Available Gestures

```python
import requests

agent_id = "A78WKV4515"
headers = {"api-secret": os.getenv("BITHUMAN_API_SECRET")}

resp = requests.get(
    f"https://api.bithuman.ai/v1/dynamics/{agent_id}",
    headers=headers,
)
gestures = resp.json()["data"].get("gestures", {})
print(list(gestures.keys()))
# Example: ["mini_wave_hello", "talk_head_nod_subtle", "laugh_react"]
```

### Step 2: Trigger Gestures from Keywords

<Tabs>
  <Tab title="Self-Hosted (VideoControl)">
    ```python
    from livekit.agents import AgentSession, UserInputTranscribedEvent
    from bithuman.api import VideoControl

    KEYWORD_ACTION_MAP = {
        "hello": "mini_wave_hello",
        "hi": "mini_wave_hello",
        "funny": "laugh_react",
        "laugh": "laugh_react",
        "yes": "talk_head_nod_subtle",
    }

    # Inside your entrypoint, after session.start():
    @session.on("user_input_transcribed")
    def on_transcribed(event: UserInputTranscribedEvent):
        if not event.is_final:
            return
        text = event.transcript.lower()
        for keyword, action in KEYWORD_ACTION_MAP.items():
            if keyword in text:
                asyncio.create_task(
                    avatar.runtime.push(VideoControl(action=action))
                )
                break
    ```
  </Tab>
  <Tab title="Cloud (RPC)">
    ```python
    from livekit import rtc
    import json
    from datetime import datetime

    KEYWORD_ACTION_MAP = {
        "hello": "mini_wave_hello",
        "funny": "laugh_react",
    }

    async def trigger_gesture(participant: rtc.LocalParticipant, target: str, action: str):
        await participant.perform_rpc(
            destination_identity=target,
            method="trigger_dynamics",
            payload=json.dumps({
                "action": action,
                "identity": participant.identity,
                "timestamp": datetime.utcnow().isoformat(),
            }),
        )

    # Inside your entrypoint, after session.start():
    @session.on("user_input_transcribed")
    def on_transcribed(event: UserInputTranscribedEvent):
        if not event.is_final:
            return
        text = event.transcript.lower()
        for keyword, action in KEYWORD_ACTION_MAP.items():
            if keyword in text:
                for identity in ctx.room.remote_participants.keys():
                    asyncio.create_task(
                        trigger_gesture(ctx.room.local_participant, identity, action)
                    )
                break
    ```
  </Tab>
</Tabs>

---

## Controlling the Avatar via REST API

Once an avatar is running in a room, you can control it from any backend using the REST API — no LiveKit connection needed.

### Make the Avatar Speak

```bash
curl -X POST "https://api.bithuman.ai/v1/agent/A78WKV4515/speak" \
  -H "api-secret: $BITHUMAN_API_SECRET" \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello! Welcome to our demo."}'
```

### Add Context (Silent Knowledge)

```bash
curl -X POST "https://api.bithuman.ai/v1/agent/A78WKV4515/add-context" \
  -H "api-secret: $BITHUMAN_API_SECRET" \
  -H "Content-Type: application/json" \
  -d '{
    "context": "The customer just purchased a premium plan.",
    "type": "add_context"
  }'
```

The avatar won't say this aloud, but it will use the information in future responses.

<Tip>
These REST API calls work from any language or platform — use them to integrate avatars into existing apps without touching the agent code.
</Tip>

---

## Using the SDK Without LiveKit

If you don't need real-time rooms (e.g., generating video files or building a custom UI), use the Python SDK directly:

```python
import asyncio
import cv2
from bithuman import AsyncBithuman
from bithuman.audio import load_audio, float32_to_int16

async def main():
    # Initialize the runtime
    runtime = await AsyncBithuman.create(
        model_path="avatar.imx",
        api_secret="sk_bh_...",
    )
    await runtime.start()

    # Load an audio file and push it
    audio, sr = load_audio("speech.wav")
    audio_int16 = float32_to_int16(audio)
    await runtime.push_audio(audio_int16.tobytes(), sr)
    await runtime.flush()

    # Get animated video frames
    async for frame in runtime.run():
        if frame.has_image:
            cv2.imshow("Avatar", frame.bgr_image)
            cv2.waitKey(1)

        if frame.end_of_speech:
            break

asyncio.run(main())
```

This gives you raw numpy frames — display them however you want.

---

## Complete Docker Example

For the fastest path to a working demo, use the Docker example that packages everything together:

```bash
# Clone the examples repo
git clone https://github.com/bithuman-product/examples.git
cd examples/public-docker-example

# Configure
cat > .env << 'EOF'
BITHUMAN_API_SECRET=sk_bh_...
OPENAI_API_KEY=sk-...
LIVEKIT_URL=wss://your-project.livekit.cloud
LIVEKIT_API_KEY=APIxxxxxxxx
LIVEKIT_API_SECRET=xxxxxxxx
EOF

# Add your avatar model
mkdir -p models
cp ~/Downloads/avatar.imx models/

# Launch
docker compose up
```

Open [http://localhost:4202](http://localhost:4202) to talk to your avatar.

---

## Troubleshooting

<AccordionGroup>
  <Accordion title="Avatar doesn't appear in the room">
    **Cloud mode:** Check that your `avatar_id` exists — look it up in the [bitHuman dashboard](https://www.bithuman.ai). Verify your API secret is valid with:
    ```bash
    curl -X POST https://api.bithuman.ai/v1/validate \
      -H "api-secret: $BITHUMAN_API_SECRET"
    ```

    **Self-hosted mode:** Check that the `.imx` file path is correct and the file is not corrupted:
    ```bash
    bithuman validate --model-path /path/to/avatar.imx
    ```
  </Accordion>

  <Accordion title="Avatar appears but no lip movement">
    The avatar needs audio input to animate. Ensure:
    1. Your TTS is producing audio (test with `openai.TTS()` separately)
    2. The `room_output=avatar` parameter is set in `session.start()`
    3. Check agent logs for audio pipeline errors
  </Accordion>

  <Accordion title="'Authentication failed' error">
    - Verify your API secret is correct (copy-paste from dashboard)
    - Check you have credits remaining in your account
    - Ensure the `BITHUMAN_API_SECRET` environment variable is set
  </Accordion>

  <Accordion title="High latency / slow first frame">
    **Cloud:** First request downloads the model (~2-4 seconds). Subsequent requests use cache (~1-2 seconds).

    **Self-hosted CPU:** First load takes ~20 seconds (model initialization). Keep the process running for fast subsequent sessions.

    **Self-hosted GPU:** Cold start takes ~30-40 seconds. Use long-running containers with preset avatars for ~4 second startup.
  </Accordion>

  <Accordion title="'No available workers' or 503 errors">
    All avatar workers are busy. The system retries automatically (up to 5 times with backoff). If it persists:
    - Check your usage limits
    - Try again in a few seconds
    - For self-hosted: increase the number of worker replicas
  </Accordion>
</AccordionGroup>

---

## Next Steps

<CardGroup cols={3}>
  <Card title="Dynamics API" icon="person-running" href="/api-reference/dynamics">
    Add gestures and movements
  </Card>
  <Card title="Webhooks" icon="bell" href="/integrations/webhooks">
    Get notified about session events
  </Card>
  <Card title="Embed Avatars" icon="code" href="/integrations/embed">
    Put avatars on any website
  </Card>
</CardGroup>
